import os
import json
import re
import argparse
from collections import OrderedDict


def generate_dataset_entries(directory, base_path):
    """
    æ‰«æç›®å½•ï¼Œä¸ºæ¯ä¸ªåŒ¹é…çš„jsonæ–‡ä»¶ç”Ÿæˆæ¡ç›®ã€‚

    Args:
        directory (str): è¦æ‰«æçš„åŒ…å«jsonæ–‡ä»¶çš„ç›®å½•ã€‚
        base_path (str): åœ¨ç”Ÿæˆçš„jsonä¸­ï¼Œç”¨ä½œæ–‡ä»¶è·¯å¾„å‰ç¼€çš„å­—ç¬¦ä¸²ã€‚

    Returns:
        dict: ä¸€ä¸ªåŒ…å«æ–°ç”Ÿæˆæ¡ç›®çš„å­—å…¸ã€‚
    """
    # æ­£åˆ™è¡¨è¾¾å¼ï¼Œç”¨äºä»æ–‡ä»¶åä¸­æå–ç®—æ³•åç§°å’Œç§å­ç¼–å·
    # ä¾‹å¦‚: 0429_..._A2C_seed1.json -> æå– "A2C" å’Œ "1"
    pattern = re.compile(r'_([A-Z0-9]+)_seed(\d+)\.json$')

    new_entries = {}
    print(f"ğŸ” æ­£åœ¨æ‰«æç›®å½•: {directory}")

    # éå†ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
    for filename in sorted(os.listdir(directory)):
        if filename.endswith('.json'):
            match = pattern.search(filename)
            if match:
                # æå–ç®—æ³•å’Œç§å­ID
                algo = match.group(1)
                seed = match.group(2)

                # æ„å»ºå”¯ä¸€çš„é”®ï¼Œä¾‹å¦‚ "A2C_seed1"
                unique_key = f"{algo}_seed{seed}"

                # æ„å»ºæ–‡ä»¶è·¯å¾„å€¼ï¼Œä½¿ç”¨ os.path.join ä¿è¯è·¯å¾„åˆ†éš”ç¬¦æ­£ç¡®
                # åœ¨Windowsä¸Šä¼šæ˜¯ \ï¼Œåœ¨Linuxä¸Šä¼šæ˜¯ /
                file_path_value = os.path.join(base_path, filename).replace("\\", "/")  # ä¿è¯è¾“å‡ºä¸º a/b/c æ ¼å¼

                # åˆ›å»ºæ¡ç›®
                new_entries[unique_key] = {"file_name": file_path_value}
                print(f"  âœ… åŒ¹é…æˆåŠŸ: {filename} -> {unique_key}")
            else:
                print(f"  âš ï¸  è·³è¿‡æ–‡ä»¶ (æ ¼å¼ä¸åŒ¹é…): {filename}")

    return new_entries


def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºè§£æå‘½ä»¤è¡Œå‚æ•°å¹¶æ‰§è¡Œæ›´æ–°é€»è¾‘ã€‚"""
    parser = argparse.ArgumentParser(
        description="è‡ªåŠ¨æ‰«æç›®å½•ä¸­çš„JSONæ–‡ä»¶ï¼Œå¹¶æ›´æ–°æˆ–åˆ›å»º dataset_info.json æ–‡ä»¶ã€‚",
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument(
        '-d', '--directory',
        required=True,
        help="åŒ…å«æºJSONæ–‡ä»¶çš„ç›®æ ‡ç›®å½•ã€‚\nä¾‹å¦‚: /path/to/your/json_files"
    )
    parser.add_argument(
        '-b', '--base_path',
        required=True,
        help="åœ¨è¾“å‡ºçš„JSONæ–‡ä»¶ä¸­ï¼Œä¸ºæ¯ä¸ªæ–‡ä»¶è·¯å¾„æŒ‡å®šçš„å‰ç¼€ã€‚\nä¾‹å¦‚: spbs_rl_data/0710_sft_spbsrl"
    )
    parser.add_argument(
        '-o', '--output_file',
        default='dataset_info.json',
        help="è¦æ›´æ–°æˆ–åˆ›å»ºçš„è¾“å‡ºJSONæ–‡ä»¶çš„è·¯å¾„ã€‚\n(é»˜è®¤ä¸º: dataset_info.json)"
    )

    args = parser.parse_args()

    # 1. å¦‚æœè¾“å‡ºæ–‡ä»¶å­˜åœ¨ï¼Œåˆ™è¯»å–ç°æœ‰æ•°æ®
    existing_data = OrderedDict()
    if os.path.exists(args.output_file):
        try:
            with open(args.output_file, 'r', encoding='utf-8') as f:
                existing_data = json.load(f, object_pairs_hook=OrderedDict)
            print(f"ğŸ“– å·²æˆåŠŸè¯»å–ç°æœ‰æ–‡ä»¶: {args.output_file}")
        except json.JSONDecodeError:
            print(f"ğŸš¨ é”™è¯¯: ç°æœ‰æ–‡ä»¶ {args.output_file} ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„JSONæ–‡ä»¶ã€‚è„šæœ¬å°†ä¸­æ–­ã€‚")
            return
    else:
        print(f"â„¹ï¸  è¾“å‡ºæ–‡ä»¶ {args.output_file} ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶ã€‚")

    # 2. ä»ç›®æ ‡ç›®å½•ç”Ÿæˆæ–°çš„æ¡ç›®
    new_entries = generate_dataset_entries(args.directory, args.base_path)

    if not new_entries:
        print("ğŸ”š åœ¨æŒ‡å®šç›®å½•ä¸­æœªæ‰¾åˆ°åŒ¹é…çš„JSONæ–‡ä»¶ã€‚æœªåšä»»ä½•æ›´æ”¹ã€‚")
        return

    # 3. åˆå¹¶æ•°æ®ï¼šå°†æ–°æ¡ç›®æ·»åŠ åˆ°ç°æœ‰æ•°æ®ä¸­ï¼ˆå¦‚æœé”®å·²å­˜åœ¨ï¼Œåˆ™ä¼šè¦†ç›–ï¼‰
    existing_data.update(new_entries)

    # 4. å°†åˆå¹¶åçš„æ•°æ®å†™å›æ–‡ä»¶
    try:
        with open(args.output_file, 'w', encoding='utf-8') as f:
            # indent=2 ä½¿JSONæ–‡ä»¶æ ¼å¼åŒ–ï¼Œæ˜“äºé˜…è¯»
            # ensure_ascii=False ä¿è¯ä¸­æ–‡å­—ç¬¦æ­£å¸¸æ˜¾ç¤º
            json.dump(existing_data, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ‰ æˆåŠŸ! æ•°æ®å·²å†™å…¥åˆ°: {args.output_file}")
    except IOError as e:
        print(f"ğŸš¨ é”™è¯¯: æ— æ³•å†™å…¥æ–‡ä»¶ {args.output_file}ã€‚é”™è¯¯ä¿¡æ¯: {e}")


if __name__ == '__main__':
    main()
